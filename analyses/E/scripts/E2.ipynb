{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96a33ff-40a6-44ed-9aaa-436a0dc84982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n"
     ]
    }
   ],
   "source": [
    "# Python script for analyses of \"Antibody affinity birth through somatic hypermutation\" publication.\n",
    "# This pipeline is divided into X sections. At the beginning of each section there is a comment which indicates which figures of the publication are generated based on that section.\n",
    "\n",
    "# input sequences for these analyses are uploaded in data folder. By a successful run, the result of each section will be saved in output folder.\n",
    "print('Running...')\n",
    "import re\n",
    "import operator\n",
    "\n",
    "import os\n",
    "#import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "#import logomaker #https://logomaker.readthedocs.io\n",
    "\n",
    "# Functions\n",
    "def display_big():\n",
    "\n",
    "    # df = pd.DataFrame()\n",
    "    # pd.options.display.max_colwidth = 2000\n",
    "    pd.set_option('display.max_rows', 10)\n",
    "    pd.set_option('display.max_columns', 500)\n",
    "    pd.set_option('display.width', 1000)\n",
    "\n",
    "display_big()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f60c3d1b-a1c5-4d6c-946b-41fc40b8b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder='../data'\n",
    "\n",
    "input_folder = os.getenv('VAR_IN_FOLDER', f\"{data_folder}/input\")\n",
    "output_folder = os.getenv('VAR_OUT_FOLDER', f\"{data_folder}/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd0338a0-9a67-48f5-bb0b-f9b175c9c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_output_folder(section_output):\n",
    "    output_folder=data_folder+'/output/'+section_output\n",
    "\n",
    "    if not os.path.isdir(output_folder): # make output folder if it doesn't exist\n",
    "        os.makedirs(output_folder)\n",
    "    return(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9138cbc4-c997-4be4-ab6d-820e6ae7d28a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Section1: preparation\n",
    "\n",
    "output_folder_prep=set_output_folder('1_prep')\n",
    "output_folder_num_miss=set_output_folder('2_num_miss')\n",
    "output_folder_freq_pos=set_output_folder('3_freq_per_position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10dff0b9-2343-4d0d-9da8-0ce46cd6b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_expanded_nts_included=pd.read_csv('{}/../1_prep/dfs_expanded_nts_included.tsv'.format(output_folder_prep), sep='\\t', header=0, low_memory=False)\n",
    "dfs_expanded_nts_included.reset_index(inplace=True, drop=True)\n",
    "\n",
    "dfs_expanded_nts_excluded=pd.read_csv('{}/../1_prep/dfs_expanded_nts_excluded.tsv'.format(output_folder_prep), sep='\\t', header=0, low_memory=False)\n",
    "dfs_expanded_nts_excluded.reset_index(inplace=True, drop=True)\n",
    "\n",
    "dfs_expanded_aas_included=pd.read_csv('{}/../1_prep/dfs_expanded_aas_included.tsv'.format(output_folder_prep), sep='\\t', header=0, low_memory=False)\n",
    "dfs_expanded_aas_included.reset_index(inplace=True, drop=True)\n",
    "\n",
    "dfs_expanded_aas_excluded=pd.read_csv('{}/../1_prep/dfs_expanded_aas_excluded.tsv'.format(output_folder_prep), sep='\\t', header=0, low_memory=False)\n",
    "dfs_expanded_aas_excluded.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ce4f0f-0719-4208-8383-6f64421645fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_nt_miss(df):\n",
    "    nt_miss_list=[]\n",
    "\n",
    "    for chain in ['VH', 'VL']:\n",
    "        df_per_chain=df[df['chain']==chain]\n",
    "        \n",
    "        grouping=df_per_chain.groupby(by=['status', 'mouse', 'dataset', 'chain', 'sub_dataset'])\n",
    "        #grouping=df_per_chain.groupby(by=['mouse', 'dataset'])\n",
    "        dfs_chain=pd.DataFrame()\n",
    "\n",
    "        for grouped, df_chain in grouping:\n",
    "            ID='_'.join(grouped)\n",
    "            #if ID=='Published_B18_Passenger_VH_NA':\n",
    "            # print(grouped)\n",
    "            df_chain.rename(columns={'nt_miss':ID}, inplace=True)\n",
    "            df_chain=df_chain[[ID]].sort_values(by=ID)\n",
    "            df_chain.reset_index(inplace=True, drop=True)\n",
    "            #display(df_chain)\n",
    "            \n",
    "            dfs_chain=pd.concat([dfs_chain, df_chain], axis=1)\n",
    "        nt_miss_list.append(dfs_chain)\n",
    "    return(nt_miss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35edff70-bfc9-4a25-8adc-45debc4fbb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_aa_miss(df):\n",
    "    aa_miss_list=[]\n",
    "\n",
    "    for chain in ['VH', 'VL']:\n",
    "        df_per_chain=df[df['chain']==chain]\n",
    "        grouping=df_per_chain.groupby(by=['status', 'mouse', 'dataset', 'chain', 'sub_dataset'])\n",
    "        #grouping=df_per_chain.groupby(by=['mouse', 'dataset'])\n",
    "        dfs_chain=pd.DataFrame()\n",
    "\n",
    "        for grouped, df_chain in grouping:\n",
    "            ID='_'.join(grouped)\n",
    "            # print(ID)\n",
    "            df_chain.rename(columns={'aa_miss':ID}, inplace=True)\n",
    "            df_chain=df_chain[[ID]].sort_values(by=ID)\n",
    "            df_chain.reset_index(inplace=True, drop=True)\n",
    "            dfs_chain=pd.concat([dfs_chain, df_chain], axis=1)\n",
    "        aa_miss_list.append(dfs_chain)\n",
    "    return(aa_miss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c9a53ec-dd94-44f2-90cd-81dfb5e5f641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_nt_miss_list_included=num_nt_miss(dfs_expanded_nts_included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7868038e-b0fd-4142-a737-d936c58eb072",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nt_miss_list_excluded=num_nt_miss(dfs_expanded_nts_excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db8de7e5-8579-4b81-9923-5935fe352d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_aa_miss_list_included=num_aa_miss(dfs_expanded_aas_included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76f8ff87-c887-41fa-9f84-5642af8902a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_aa_miss_list_excluded=num_aa_miss(dfs_expanded_aas_excluded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6ca763-1ec9-4bd1-90cb-d254a80a30ac",
   "metadata": {},
   "source": [
    "# Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "456f5bf4-55e3-4f25-9fa0-714767f8b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sorting order for each block\n",
    "block1_order = ['Unimmunized', 'EarlyGC', 'LateGC', 'Published']\n",
    "block2_order = ['B18-383', 'HA-uMT', 'HA-WT', 'HRO']\n",
    "block3_order = ['OVA', 'APC', 'CGG', 'OVA-CTLA4', 'OVA-Isotype', 'CGG-CTLA4', 'CGG-Isotype', 'BM', 'SPL', 'PP', 'MLN']\n",
    "block4_order = ['VH', 'VL']\n",
    "block5_order = ['NA', '0-1', '1-1', '1-100', '1-1000', 'rep1', 'rep2']\n",
    "\n",
    "# Create dictionaries for fast lookups of the sort order\n",
    "block1_dict = {v: i for i, v in enumerate(block1_order)}\n",
    "block2_dict = {v: i for i, v in enumerate(block2_order)}\n",
    "block3_dict = {v: i for i, v in enumerate(block3_order)}\n",
    "block4_dict = {v: i for i, v in enumerate(block4_order)}\n",
    "block5_dict = {v: i for i, v in enumerate(block5_order)}\n",
    "\n",
    "# Function to get the sort key for each entry\n",
    "def sort_key(x):\n",
    "    # Split the input string into its 5 components\n",
    "    blocks = x.split('_')\n",
    "    \n",
    "    # Ensure each block has a valid entry, fill with a high number if not found\n",
    "    b1 = block1_dict.get(blocks[0], float('inf'))\n",
    "    b2 = block2_dict.get(blocks[1], float('inf'))\n",
    "    b3 = block3_dict.get(blocks[2], float('inf'))\n",
    "    b4 = block4_dict.get(blocks[3], float('inf'))\n",
    "    b5 = block5_dict.get(blocks[4], float('inf'))\n",
    "    \n",
    "    # Return a tuple that Python will use to sort the items\n",
    "    return (b1, b2, b3, b4, b5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89d80380-7b99-46e9-a9ce-5a24c255858e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.0\n",
      "33.0\n"
     ]
    }
   ],
   "source": [
    "num_nt_miss_vh=num_nt_miss_list_included[0][sorted(set(num_nt_miss_list_included[0]), key=sort_key)]\n",
    "num_nt_miss_vh.to_csv('{}/nt_miss_included_vh.tsv'.format(output_folder_num_miss), sep = '\\t', index=False)\n",
    "print(num_nt_miss_vh.max().max())\n",
    "num_nt_miss_vl=num_nt_miss_list_included[1][sorted(set(num_nt_miss_list_included[1]), key=sort_key)]\n",
    "num_nt_miss_vl.to_csv('{}/nt_miss_included_vl.tsv'.format(output_folder_num_miss), sep = '\\t', index=False)\n",
    "print(num_nt_miss_vl.max().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cccbdf0-ba7c-4e76-b9fc-9bf92e89ff8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.0\n",
      "33.0\n"
     ]
    }
   ],
   "source": [
    "num_nt_miss_vh=num_nt_miss_list_excluded[0][sorted(set(num_nt_miss_list_excluded[0]), key=sort_key)]\n",
    "num_nt_miss_vh.to_csv('{}/nt_miss_excluded_vh.tsv'.format(output_folder_num_miss), sep = '\\t', index=False)\n",
    "print(num_nt_miss_vh.max().max())\n",
    "num_nt_miss_vl=num_nt_miss_list_excluded[1][sorted(set(num_nt_miss_list_excluded[1]), key=sort_key)]\n",
    "num_nt_miss_vl.to_csv('{}/nt_miss_excluded_vl.tsv'.format(output_folder_num_miss), sep = '\\t', index=False)\n",
    "print(num_nt_miss_vl.max().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57ec7149-695f-49a2-87aa-33912468cc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.0\n",
      "24.0\n"
     ]
    }
   ],
   "source": [
    "num_aa_miss_vh=num_aa_miss_list_included[0][sorted(set(num_aa_miss_list_included[0]), key=sort_key)]\n",
    "num_aa_miss_vh.to_csv('{}/aa_miss_included_vh.tsv'.format(output_folder_num_miss), sep = '\\t', index=False)\n",
    "print(num_aa_miss_vh.max().max())\n",
    "num_aa_miss_vl=num_aa_miss_list_included[1][sorted(set(num_aa_miss_list_included[1]), key=sort_key)]\n",
    "num_aa_miss_vl.to_csv('{}/aa_miss_included_vl.tsv'.format(output_folder_num_miss), sep = '\\t', index=False)\n",
    "print(num_aa_miss_vl.max().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7547aac1-4551-4127-ba7d-66789f92edcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.0\n",
      "24.0\n"
     ]
    }
   ],
   "source": [
    "num_aa_miss_vh=num_aa_miss_list_excluded[0][sorted(set(num_aa_miss_list_excluded[0]), key=sort_key)]\n",
    "num_aa_miss_vh.to_csv('{}/aa_miss_excluded_vh.tsv'.format(output_folder_num_miss), sep = '\\t', index=False)\n",
    "print(num_aa_miss_vh.max().max())\n",
    "num_aa_miss_vl=num_aa_miss_list_excluded[1][sorted(set(num_aa_miss_list_excluded[1]), key=sort_key)]\n",
    "num_aa_miss_vl.to_csv('{}/aa_miss_excluded_vl.tsv'.format(output_folder_num_miss), sep = '\\t', index=False)\n",
    "print(num_aa_miss_vl.max().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbbb271-060d-4737-a143-9f18c5d895d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
